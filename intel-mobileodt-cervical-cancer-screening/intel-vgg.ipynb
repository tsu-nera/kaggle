{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel & MobileODT Cervical Cancer Screening\n",
    "* [Intel & MobileODT Cervical Cancer Screening | Kaggle](https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"/home/tsu-nera/repo/kaggle/input/intel-mobileodt-cervical-cancer-screening/\"\n",
    "path=\"/input/\"\n",
    "width = 100\n",
    "height = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(path+\"resized\", exist_ok=True)\n",
    "\n",
    "for filetype in [\"train/\", \"valid/\"]:\n",
    "    for category in [\"Type_1\", \"Type_2\", \"Type_3\"]:\n",
    "        os.makedirs(path+\"resized/\"+filetype+category,  exist_ok=True)\n",
    "os.makedirs(path+\"resized/test/unknown\",  exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def resize_img(src, dst, width, height):\n",
    "    img = Image.open(src, 'r')\n",
    "    resized = img.resize((width, height))\n",
    "    resized.save(dst, 'JPEG', optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "categories = ['Type_1/', 'Type_2/', 'Type_3/']\n",
    "filetypes = [\"train/\"]\n",
    "\n",
    "for filetype in filetypes:\n",
    "    for category in categories:\n",
    "            globpath = filetype+category+\"*.jpg\"\n",
    "            files = glob.glob(globpath)\n",
    "            for file in files:\n",
    "                src = path+file\n",
    "                dst = path+\"resized/\"+file\n",
    "                resize_img(src, dst, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globpath = \"test/unknown/*.jpg\"\n",
    "files = glob.glob(globpath)\n",
    "for file in files:\n",
    "    src = path+file\n",
    "    dst = path+\"resized/\"+file\n",
    "    resize_img(src, dst, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create train/valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/tsu-nera/repo/kaggle/input/intel-mobileodt-cervical-cancer-screening/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p valid/Type_1\n",
    "%mkdir -p valid/Type_2\n",
    "%mkdir -p valid/Type_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $path\n",
    "%cd resized/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "g = glob.glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(81): \n",
    "    shutil.move(shuf[i], '../valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(\n",
    "                rescale=1. / 255,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True)\n",
    "gen = image.ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1181 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n",
      "Found 512 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = gen_t.flow_from_directory(path+'train', batch_size=batch_size, target_size=(width,height))\n",
    "val_batches = gen.flow_from_directory(path+'valid', batch_size=batch_size, target_size=(width, height))\n",
    "test_batches = gen.flow_from_directory(path+'test', batch_size=batch_size, target_size=(width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(width,height,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  finetune\n",
    "* [Fine-tuning pre-trained VGG16 not possible since `add` method is not defined for `Model` class? · Issue #4040 · fchollet/keras](https://github.com/fchollet/keras/issues/4040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf = 128\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.output_shape\n",
    "last = base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "\n",
    "x = BatchNormalization(axis=1)(last)\n",
    "x = Conv2D(nf,3,3, border_mode='same')(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "#x = MaxPooling2D(dim_ordering=\"th\")(x)\n",
    "x = Conv2D(nf,3,3, border_mode='same')(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "#x = MaxPooling2D(dim_ordering=\"th\")(x)\n",
    "x = Conv2D(nf,3,3, border_mode='same')(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = MaxPooling2D((1,2))(x)\n",
    "x = Conv2D(3,3,3, border_mode='same')(x)\n",
    "x = Dropout(p)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "preds = Activation('softmax')(x)\n",
    "\n",
    "model = Model(base_model.input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(last)\n",
    "x = Dense(512)(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512)(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(base_model.input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Nadam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大量のWarningがでてkernel が死ぬときの解決法\n",
    "* [Intel & MobileODT Cervical Cancer Screening | Kaggle](https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/discussion/31558)\n",
    "\n",
    "これでも死んだ。画像サイズがおおきいのかな？？ リサイズしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "escb = EarlyStopping(monitor=\"loss\", patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Allocation Errorで実行できない。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "73/73 [==============================] - 13s - loss: 1.7823 - acc: 0.4109 - val_loss: 1.2111 - val_acc: 0.4648\n",
      "Epoch 2/30\n",
      "73/73 [==============================] - 10s - loss: 1.3542 - acc: 0.4793 - val_loss: 1.9799 - val_acc: 0.4859\n",
      "Epoch 3/30\n",
      "73/73 [==============================] - 10s - loss: 1.1852 - acc: 0.5284 - val_loss: 1.4943 - val_acc: 0.4155\n",
      "Epoch 4/30\n",
      "73/73 [==============================] - 10s - loss: 1.0662 - acc: 0.5389 - val_loss: 1.2133 - val_acc: 0.4754\n",
      "Epoch 5/30\n",
      "73/73 [==============================] - 10s - loss: 1.0262 - acc: 0.5339 - val_loss: 1.0606 - val_acc: 0.5563\n",
      "Epoch 6/30\n",
      "73/73 [==============================] - 10s - loss: 0.9850 - acc: 0.5426 - val_loss: 1.0207 - val_acc: 0.5458\n",
      "Epoch 7/30\n",
      "73/73 [==============================] - 10s - loss: 0.9718 - acc: 0.5517 - val_loss: 1.1802 - val_acc: 0.4824\n",
      "Epoch 8/30\n",
      "73/73 [==============================] - 10s - loss: 0.9093 - acc: 0.5768 - val_loss: 1.0282 - val_acc: 0.5106\n",
      "Epoch 9/30\n",
      "73/73 [==============================] - 10s - loss: 0.9171 - acc: 0.5867 - val_loss: 1.0948 - val_acc: 0.5211\n",
      "Epoch 10/30\n",
      "73/73 [==============================] - 10s - loss: 0.9045 - acc: 0.6007 - val_loss: 1.0735 - val_acc: 0.4859\n",
      "Epoch 11/30\n",
      "73/73 [==============================] - 10s - loss: 0.8973 - acc: 0.5810 - val_loss: 0.9977 - val_acc: 0.5387\n",
      "Epoch 12/30\n",
      "73/73 [==============================] - 10s - loss: 0.8877 - acc: 0.6178 - val_loss: 1.1692 - val_acc: 0.4296\n",
      "Epoch 13/30\n",
      "73/73 [==============================] - 10s - loss: 0.8762 - acc: 0.5923 - val_loss: 1.1419 - val_acc: 0.5035\n",
      "Epoch 14/30\n",
      "73/73 [==============================] - 10s - loss: 0.8691 - acc: 0.6296 - val_loss: 1.0883 - val_acc: 0.5070\n",
      "Epoch 15/30\n",
      "73/73 [==============================] - 10s - loss: 0.8414 - acc: 0.6388 - val_loss: 0.9737 - val_acc: 0.5211\n",
      "Epoch 16/30\n",
      "73/73 [==============================] - 10s - loss: 0.8638 - acc: 0.6187 - val_loss: 1.1470 - val_acc: 0.5246\n",
      "Epoch 17/30\n",
      "73/73 [==============================] - 10s - loss: 0.8225 - acc: 0.6334 - val_loss: 1.1811 - val_acc: 0.5106\n",
      "Epoch 18/30\n",
      "73/73 [==============================] - 11s - loss: 0.7941 - acc: 0.6583 - val_loss: 1.0709 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "73/73 [==============================] - 11s - loss: 0.8183 - acc: 0.6313 - val_loss: 1.0543 - val_acc: 0.5528\n",
      "Epoch 20/30\n",
      "73/73 [==============================] - 10s - loss: 0.7622 - acc: 0.6544 - val_loss: 1.2087 - val_acc: 0.5141\n",
      "Epoch 21/30\n",
      "73/73 [==============================] - 11s - loss: 0.8537 - acc: 0.6245 - val_loss: 1.1792 - val_acc: 0.4542\n",
      "Epoch 22/30\n",
      "73/73 [==============================] - 10s - loss: 0.7586 - acc: 0.6622 - val_loss: 1.1263 - val_acc: 0.5106\n",
      "Epoch 23/30\n",
      "73/73 [==============================] - 10s - loss: 0.7744 - acc: 0.6516 - val_loss: 1.1379 - val_acc: 0.5387\n",
      "Epoch 24/30\n",
      "73/73 [==============================] - 10s - loss: 0.8218 - acc: 0.6281 - val_loss: 1.1654 - val_acc: 0.4542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0300878240>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.n//batch_size, epochs=30,\n",
    "                    validation_data=val_batches, validation_steps=val_batches.n//batch_size, callbacks=[escb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_batches, test_batches.n//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/2, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filenames = [a[8:] for a in test_batches.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Type_2</th>\n",
       "      <th>Type_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>0.140263</td>\n",
       "      <td>0.847545</td>\n",
       "      <td>0.012192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>377.jpg</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.028148</td>\n",
       "      <td>0.967293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296.jpg</td>\n",
       "      <td>0.096546</td>\n",
       "      <td>0.749115</td>\n",
       "      <td>0.154339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>458.jpg</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.743163</td>\n",
       "      <td>0.220064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.jpg</td>\n",
       "      <td>0.290981</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>0.011516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name    Type_1    Type_2    Type_3\n",
       "0     10.jpg  0.140263  0.847545  0.012192\n",
       "1    377.jpg  0.005000  0.028148  0.967293\n",
       "2    296.jpg  0.096546  0.749115  0.154339\n",
       "3    458.jpg  0.036773  0.743163  0.220064\n",
       "4     89.jpg  0.290981  0.697503  0.011516"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image_name', test_filenames)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/home/tsu-nera/repo/kaggle/intel-mobileodt-cervical-cancer-screening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/output/submission.csv"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "31px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
